<!doctype html>
<html lang="en">
<head>
<title>Mass Editing Memory in a Transformer</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="description" content="Mass Editing Memory in a Transformer" />
<meta property="og:title" content="Mass Editing Memory in a Transformer" />
<meta property="og:url" content="https://rome.baulab.info/" />
<meta property="og:image" content="https://rome.baulab.info/images/ct-thumb.gif" />
<meta property="og:description" content="Cracking open the black box of huge autoregressive transformer neural network language models." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="twitter:title" content="Mass Editing Memory in a Transformer" />
<meta name="twitter:description" content="Cracking open the black box of huge autoregressive transformer neural network language models." />
<meta name="twitter:image" content="https://rome.baulab.info/images/ct-thumb.gif" />
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

<style>
.relatedthumb {
  float:left; width: 200px; margin: 3px 10px 7px 0;
}
.relatedblock {
  clear: both;
  display: inline-block;
}
.bold-sc {
  font-variant: small-caps;
  font-weight: bold;
}
.cite, .citegroup {
  margin-bottom: 8px;
}
:target {
  background-color: yellow;
}
</style>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
</script>

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Mass Editing</nobr>
 <nobr class="widenobr">Memory in a Transformer</nobr>
 </h1>
<address>
  <nobr><a href="https://mengk.me/" target="_blank"
  >Kevin Meng</a><sup>1</sup>,</nobr>
  <nobr><a href="https://arnab-api.github.io/" target="_blank"
  >Arnab Sen Sharma</a><sup>2</sup>,</nobr>
  <nobr><a href="https://www.alexandonian.com/" target="_blank"
  >Alex Andonian</a><sup>1</sup>,</nobr>
  <nobr><a href="https://www.cs.technion.ac.il/~belinkov/" target="_blank"
  >Yonatan Belinkov</a><sup>3</sup>,</nobr>
  <nobr><a href="https://baulab.info/" target="_blank"
  >David Bau</a><sup>2</sup></nobr>
 <br>
  <nobr><sup>1</sup><a href="https://www.csail.mit.edu/" target="_blank"
  >MIT CSAIL</a>,</nobr>
  <nobr><sup>2</sup><a href="https://khoury.northeastern.edu/" target="_blank"
  >Northeastern University</a>,</nobr>
  <nobr><sup>3</sup><a href="https://www.cs.technion.ac.il/" target="_blank"
  >Technion - IIT</a></nobr>;
  <nobr><sup>*</sup><a class="likelink">Equal Contribution</a></nobr>
</address>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row justify-content-center text-center">

<p>
<a href="https://arxiv.org/pdf/2202.05262.pdf" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/paper-thumb.png" style="border:1px solid" alt="ArXiv Preprint thumbnail" data-nothumb=""><br>ArXiv<br>Preprint</a>
<a href="https://github.com/kmeng01/rome" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/code-thumb.png" style="border:1px solid" alt="Github code thumbnail" data-nothumb=""><br>Source Code<br>Github</a>
</p>

<div class="card" style="max-width: 1020px;">
<div class="card-block">
<!--
<h3>Can Facts be Located in a Large Neural Network?</h3>
<p>
Can knowledge be located within particular neurons
within a huge deep network?  We put GPT-style autoregressive
transformer language models under the microscope, and we find evidence
for a computational knowledge center in the network. This insight leads
to a state-of-the art method for editing a GPT model's factual knowledge.
</p>
-->
<h3>How many memories can be added to deep network's weights?</h3>
<p>
<p>Large language model contain <em>implicit</em> knowledge with no
built-in way to enuemrate or update that knowledge.
In this paper, we take on the update problem, asking how hundreds or
thousands of factual associations can be edited at once.
</p>
</div><!--card-block-->
</div><!--card-->

</div><!--row-->

<div class="row">
<div class="col">
<h2>Why Edit Knowledge in a Model?</h2>

<p>One of the classical problems in artificial intelligence is the creation of
a <em>knowledge base</em> that can retrieve knowledge about facts in the real world,
such as ("Michael Jordan", "plays the sport") &rightarrow; "basketball".
Since massive language models are able to answer such factual questions flexibly
using natural language, they are candidates for use as a new kind of
knowledge base.

<!-- <p>However, even very large language models are known to
  <ul>
    <li> lack specialized knowledge.
    <li> have biases
    <li> recall obsolete information 
  </ul>
</p> -->

<p>However, these language models will recall obsolete information from the point in time when the training data was crawled.
The naive solution is to train the model periodically with updated datasets. But, that is infeasible given the amount of time 
and compute resources required to train a model from scratch. Recently, some <em>knowledge-editing</em> methods have been
proposed to insert new memories directly into model parameters. However, most of this line of work is focused on updating/adding a single
memory into the model. In practice we may want to insert <em>thousands</em> of new memories in order to update or treat the model, but a 
naive sequential approach of the current state-of-the-art models fails to scale up. In this work, we propose <b>MEMIT, which is capable
of updating thousands of memories at once!</b> 
</p>

<figure>
  <img src="images/Paper/MEMIT-graph-crop.png" class="center_image">
  <figcaption>Fig.1 - (a) Language models can be viewed as knowledge bases containing memorized tuples (s, r, o), each connecting some 
    subject <b><i>s</i></b> to an object <b><i>o</i></b> via a relation <b><i>r</i></b>, e.g., (s = Michael Jordan, r = plays sport, o = basketball). 
    (b) MEMIT modifies transformer weights to edit memories, e.g., “Michael Jordan now plays the sport baseball,” while (c) maintaining generalization, 
    specificity, and fluency at scales beyond other methods. As Section 5.2.2 details, editing score is the harmonic mean of efficacy, generalization, 
    and specificity metrics.
  </figcaption>
</figure>

<br/>
<p><b>MEMIT</b> is a successor to our previous work <a href="https://rome.baulab.info/">ROME</a>, which performs a <em>rank-one</em> modification of the 
  MLP weights of a single layer to directly write a memory into the model. MEMIT builds upon ROME to insert many memories by modifying the MLP weights 
  of a <em>range of</em> critical layers. We perform causal tracing to find a set of mediating MLP layers that recall memories about a certain subject.
  For GPT-J those layers are <b>&#8475;</b> = {3, 4, 5, 6, 7, 8}.
</p>

<figure>
  <img src="images/Paper/mrome-architecture-crop.png" class="center_image">
</figure>

<br/>
<p>Then for a set of new memories we calculate the update <b>&#916;</b> and spread this <b>&#916;</b> across all the mediating MLP layers such that at 
the final layer the output of final mediating layer captures all the new memories.
</p>

<figure>
  <img src="images/Paper/mrome-update-crop.png" class="center_image">
</figure>

<h2>How to Cite</h2>

<p>This work is not yet peer-reviewed. The preprint can be cited as follows.
</p>

<div class="card">
<h3 class="card-header">bibliography</h3>
<div class="card-block">
<p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. "<em>Mass Editing Memory in a Transformer.</em>" arXiv preprint <nobr><a href="https://arxiv.org/abs/">(id to come)</a> (2022).</nobr>
</p>
</div>
<h3 class="card-header">bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
@article{meng2022l,
  title={Mass Editing Memory in a Transformer},
  author={Kevin Meng and Arnab Sen Sharma and Alex Andonian and Yonatan Belinkov and David Bau},
  journal={arXiv preprint arXiv:},
  year={2022}
}
</pre>
</div>
</div>
</p>

</div>
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://baulab.info/">About the Bau Lab</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>

