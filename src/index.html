<!doctype html>
<html lang="en">
<head>
<title>Mass Editing Memory in a Transformer</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="description" content="Updating thousands of memories in GPT by directly calculating parameter changes." />
<meta property="og:title" content="Mass Editing Memory in a Transformer" />
<meta property="og:url" content="https://memit.baulab.info/" />
<meta property="og:image" content="https://memit.baulab.info/images/memit-thumb.png" />
<meta property="og:description" content="Updating thousands of memories in GPT by directly calculating parameter changes." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="twitter:title" content="Mass Editing Memory in a Transformer" />
<meta name="twitter:description" content="Updating thousands of memories in GPT by directly calculating parameter changes." />
<meta name="twitter:image" content="https://memit.baulab.info/images/memit-thumb.png" />
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

<style>
.relatedthumb {
  float:left; width: 200px; margin: 3px 10px 7px 0;
}
.relatedblock {
  clear: both;
  display: inline-block;
}
.bold-sc {
  font-variant: small-caps;
  font-weight: bold;
}
.cite, .citegroup {
  margin-bottom: 8px;
}
:target {
  background-color: yellow;
}
</style>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
</script>

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Mass Editing</nobr>
 <nobr class="widenobr">Memory in a Transformer</nobr>
 </h1>
<address>
  <nobr><a href="https://mengk.me/" target="_blank"
  >Kevin Meng</a><sup>1</sup>,</nobr>
  <nobr><a href="https://arnab-api.github.io/" target="_blank"
  >Arnab Sen Sharma</a><sup>2</sup>,</nobr>
  <nobr><a href="https://www.alexandonian.com/" target="_blank"
  >Alex Andonian</a><sup>1</sup>,</nobr>
  <nobr><a href="https://www.cs.technion.ac.il/~belinkov/" target="_blank"
  >Yonatan Belinkov</a><sup>3</sup>,</nobr>
  <nobr><a href="https://baulab.info/" target="_blank"
  >David Bau</a><sup>2</sup></nobr>
 <br>
  <nobr><sup>1</sup><a href="https://www.csail.mit.edu/" target="_blank"
  >MIT CSAIL</a>,</nobr>
  <nobr><sup>2</sup><a href="https://khoury.northeastern.edu/" target="_blank"
  >Northeastern University</a>,</nobr>
  <nobr><sup>3</sup><a href="https://www.cs.technion.ac.il/" target="_blank"
  >Technion - IIT</a></nobr>
</address>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row justify-content-center" style="margin-bottom: 20px">
<p class="text-center">
<a href="https://memit.baulab.us/"
   >New!  Try interacting with a MEMIT-edited GPT to see the effect of inserting hundreds of memories.</a>
</p>
</div>
<div class="row justify-content-center text-center">

<p>
<a href="https://arxiv.org/pdf/2210.07229.pdf" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/paper-thumb.png" style="border:1px solid; margin: 0 38px;" alt="ArXiv Preprint thumbnail" data-nothumb=""><br>ArXiv<br>Preprint</a>
<a href="https://memit.baulab.us/" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/memit-demo.png" style="border:1px solid; margin: 0 38px;" alt="ROME project thumbnail" data-nothumb=""><br>MEMIT-Edited<br>GPT demo</a>
<a href="https://github.com/kmeng01/memit" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;" alt="Github code thumbnail" data-nothumb=""><br>Source Code<br>Github</a>
<a href="https://rome.baulab.info/" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/rome-thumb.png" style="border:1px solid; margin: 0 38px;" alt="ROME project thumbnail" data-nothumb=""><br>Previous work:<br>ROME</a>
</p>

<div class="card" style="max-width: 1020px;">
<div class="card-block">
<h3>How many memories can be added to deep network's weights?</h3>
<p>
<p>Large language model contain implicit knowledge of facts
in the world, but they have no built-in way to update that knowledge.
In <a href="https://rome.baulab.info/">previous work (ROME)</a>
we found that memorized factual associations can be located at a specific location
in a GPT network, and we developed a way to directly edit parameters to alter
that location to change the model's knowledge of a single fact.
</p>
<p>
In this paper, we develop an improved direct editing method (MEMIT) and scale it
up to perform many edits at once. We find that we can update <b>thousands of memories</b>
simultaneously, improving on previous approaches by orders of magnitude.
</p>
</div><!--card-block-->
</div><!--card-->

</div><!--row-->

<div class="row">
<div class="col">
<figure class="center_image" style="margin-top: 30px">
  <img src="images/Paper/MEMIT-graph-crop.png" style="width:100%">
  <figcaption>(a) Language models can be viewed as knowledge bases containing memorized
    tuples (s, r, o), each connecting some subject <b><i>s</i></b> to an object
    <b><i>o</i></b> via a relation <b><i>r</i></b>, e.g., (s = Michael Jordan,
    r = plays sport, o = basketball).  (b) MEMIT modifies transformer weights to edit
    memories, e.g., "Michael Jordan now plays the sport baseball," while (c) maintaining
    generalization, specificity, and fluency at scales beyond other methods.
    As detailed in Section 5.2.2 of the paper, editing score is the harmonic mean of
    efficacy, generalization, and specificity metrics.
  </figcaption>
</figure>

<h2>Why edit knowledge in a model?</h2>

<p>Large language models such as the GPT models contain some amount of
<em>world knowledge</em> since they can recall facts about real people, places,
and things.  For example, if you ask GPT-3 to complete the sentence

<p>
<center><em>Michael Jordan plays the sport...</em></center>
</p>

<p>the model will predict <em>"<b>basketball</b>"</em>, a word that
not only is grammatically correct, but that it is also consistent with
a true fact in the real world.

<p>However, the knowledge contained in a large language model is not perfect:
even the largest models will be missing specialized knowledge, and a model
will also contain obsolete knowledge that it learned from old text.

<p style="text-align:center">
GPT-3 predicts: <em>Polaris is in the constellation <b>Ursa Minor</b></em>
  (<span style="color:green">correct!</span>)
</p>
<p style="text-align:center">
GPT-3 predicts: <em>Arneb is in the constellation of <b>Aquila</b></em>
  (<span style="color:red">incorrect - should be Lepus</span>)
</p>
<p style="text-align:center">
GPT-3 predicts: <em>The current Vice President of the United States is named <b>Mike Pence</b></em>
  (<span style="color:red">obsolete</span>)
</p>


<p>To fix such problems, several <em>knowledge-editing</em> methods
have been proposed to insert new memories directly into model parameters.
Yet most of this these methods are focused on updating a single
memory in the model, and it has been a challenge to use those methods
to update more than a handful of facts. In practice we may want to insert
hundreds or thousands of new memories in order to update or improve
a large model.

<p>In this work, we propose <b>MEMIT</b>, a direct model editing method that
is capable of updating <b>thousands of memories at once</b>.
</p>

<h2>How does it work?</h2>

<p><b>MEMIT</b> is a successor to our previous work <a href="https://rome.baulab.info/">ROME</a>, which performs a <em>rank-one</em> modification of the 
  MLP weights of a single layer to directly write a memory into the model. MEMIT builds upon ROME to insert many memories by modifying the MLP weights 
  of a <em>range of</em> critical layers. We perform causal tracing to find a set of mediating MLP layers that recall memories about a certain subject.
  For GPT-J those layers are <span style='font-family:Noto Sans Math'><b>&#x0211B;</b></span> = {3, 4, 5, 6, 7, 8}.
</p>

<figure>
  <img src="images/Paper/mrome-architecture-crop.png" class="center_image">
</figure>

<p>Then for a set of new memories we calculate the update  <span style='font-family:Noto Sans Math'><b>&#916;</b></span> and spread this  <span style='font-family:Noto Sans Math'><b>&#916;</b></span> across all the mediating MLP layers such that at 
the final layer the output of final mediating layer captures all the new memories.
</p>

<figure>
  <img src="images/Paper/mrome-update-crop.png" class="center_image">
</figure>

<p>
<a href="https://arxiv.org/pdf/2210.07229.pdf">In our paper</a>, we derive and explain the method in detail.  We conduct benchmarks testing the ability of MEMIT to scale on a variety of batch knowledge-editing tasks, and we compare our method to other approaches.  <a href="https://github.com/kmeng01/memit">Our code is open-source and available on Github</a>.
</p>

<h2>How to cite</h2>

<p>This work is not yet peer-reviewed. The preprint can be cited as follows.
</p>

<div class="card">
<h3 class="card-header">bibliography</h3>
<div class="card-block">
<p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. "<em>Mass Editing Memory in a Transformer.</em>" arXiv preprint <nobr><a href="https://arxiv.org/abs/2210.07229">arXiv:2210.07229</a> (2022).</nobr>
</p>
</div>
<h3 class="card-header">bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
@article{meng2022memit,
  title={Mass Editing Memory in a Transformer},
  author={Kevin Meng and Sen Sharma, Arnab and Alex Andonian and Yonatan Belinkov and David Bau},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}
</pre>
</div>
</div>
</p>

</div>
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://baulab.info/">About the Bau Lab</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
</script>
</html>

